# -*- coding: utf-8 -*-
"""
Spyder Editor

This is a temporary script file.
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import OrderedDict

#No of unique integer values in columns
def analyse_int(train):
    train.select_dtypes(np.int64).nunique().value_counts().sort_index().plot.bar(color='blue',figsize=(8,6),edgecolor='k')
    plt.xlabel('Number of Unique Values'); plt.ylabel('Count');

#Float values distribution

def analyse_float(train):
#for i,col in enumerate(train.select_dtypes('float')):
#    ax=plt.subplot(4,2,i+1)
#    sns.kdeplot(train[col].dropna(),ax=ax,color='blue')
#    plt.title(f'{col.cpitalize()} Distribution')
    colors=OrderedDict({1:'red',2:'orange',3:'blue',4:'green'})
    poverty_mapping=OrderedDict({1:'extreme',2:'moderate',3:'vulnerable',4:'non vulnerable'})
    k=train.select_dtypes('float').shape[1]
    for i,col in enumerate(train.select_dtypes('float')):
        ax=plt.subplot(int(np.ceil(k/2)),2,i+1)
        for poverty_level,color in colors.items():
            #plot each povery level
            sns.kdeplot(train.loc[train['Target']==poverty_level,col].dropna(),ax=ax,color=color,label=poverty_mapping[poverty_level])
        plt.title(f'{col.capitalize()} Distribution')





#Poverty distributions
def analyse_target(data):
    colors=OrderedDict({1:'red',2:'orange',3:'blue',4:'green'})
    train_labels=data.loc[((data['Target'].notnull())& (data['parentesco1']==1)),['Target','idhogar']]
    label_counts=train_labels['Target'].value_counts().sort_index()
    label_counts.plot.bar(color=colors.values())
    plt.title('Poverty level Breakdown')

#Since this is an aggregation problem we have to check that the values remain consistent across every household
#1328 with single records
def check_consistency(data):
    isunique=pd.Series(index=data.columns.tolist())
    for i in data.columns.tolist():
        alldata=data.groupby('idhogar')[i].apply(lambda x: x.nunique())
        c=len(alldata[alldata>1])
        isunique[i]=c
    isunique[isunique>0].plot.bar()
    return isunique


def get_missing(data):
    missing=pd.DataFrame(data.isnull().sum()).rename(columns={0:'total'})
    missing['percent']=missing['total']/len(data)
    return missing[missing['total']>0]

# missing value full analysis
##Fix missing values-v2a1
#ref_cols=['tipovivi1','tipovivi2','tipovivi2','tipovivi3','tipovivi4','tipovivi5']
#data.loc[data['v2a1'].isnull(),ref_cols].sum().plot.bar()
#plt.xticks([0,1,2,3,4],['owns and paid','owns and installment','rented','precarious','other'],rotation=60)
#
#
##impute missing values
#data.loc[data['tipovivi1']==1,'v2a1']=-1
##Come back for imputations
#
##Fix missing values-v18q1
#df=data.loc[data['parentesco1']==1].copy()
#df['v18q1'].value_counts().sort_index().plot.bar()
##check if these people donot own a tablet
#(data['v18q']==0).sum()
##right so impute missing values by zero
#data['v18q1'].fillna(0,inplace=True)
#
##Fix missing values rez_esc
#data.loc[((data['age'] > 19) | (data['age'] < 7)) & (data['rez_esc'].isnull()), 'rez_esc'] = 0
##We have to come back
##treat outliers
#data.loc[data['rez_esc']>5,'rez_esc']=5
#
##Analyze missing values
#
#df=data.loc[np.logical_and(data['v2a1'].isnull(),data['parentesco1']==1),'Target'].dropna()
#df.value_counts().sort_index().plot.bar()
#data['v2a1'].fillna(-1,inplace=True)
#
#df=data.loc[data['rez_esc'].isnull(),'Target'].dropna()
#df.value_counts().sort_index().plot.bar()
##based on above results
#data['rez_esc'].fillna(0,inplace=True)
#
#df=data.loc[data['rez_esc']==0,'Target'].dropna()
#df.value_counts().sort_index().plot.bar()
#
#
#
##Fix missing values meaneduc,sqbmeaned
#df=data.loc[data['parentesco1']==1,:]
#id_list=df.loc[df['meaneduc'].isnull(),'idhogar'].tolist()
#df=data.loc[data['idhogar'].isin(id_list),['edjefe','edjefa','idhogar']]
#df['total_age']=df['edjefe']+df['edjefa']
#dff=df.groupby('idhogar')['total_age'].apply(lambda x: x.mean())
#data['meanedu']=data['idhogar'].map(dff)
#data.loc[data['meaneduc'].isnull(),'meaneduc']=data['meanedu']
#data.loc[data['SQBmeaned'].isnull(),'SQBmeaned']=np.sqrt(data['meanedu'])
#data.drop(['meanedu'],1,inplace=True)


def fix_missing(data):
    data['v2a1'].fillna(0,inplace=True)
    data['v18q1'].fillna(0,inplace=True)
    data['rez_esc'].fillna(0,inplace=True)
    df=data.loc[data['parentesco1']==1,:]
    id_list=df.loc[df['meaneduc'].isnull(),'idhogar'].tolist()
    df=data.loc[data['idhogar'].isin(id_list),['edjefe','edjefa','idhogar']]
    df['total_age']=df['edjefe']+df['edjefa']
    dff=df.groupby('idhogar')['total_age'].apply(lambda x: x.mean())
    data['meanedu']=data['idhogar'].map(dff)
    data.loc[data['meaneduc'].isnull(),'meaneduc']=data['meanedu']
    data.loc[data['SQBmeaned'].isnull(),'SQBmeaned']=np.sqrt(data['meanedu'])
    data.drop(['meanedu'],1,inplace=True)
    return data
    

#SO we have dealt with missing values, now let's do some more analysis

#Bi variate analysis

def plot_categoricals(x, y, data, annotate = True):
    """Plot counts of two categoricals.
    Size is raw count for each grouping.
    Percentages are for a given value of y."""
    
    # Raw counts 
    raw_counts = pd.DataFrame(data.groupby(y)[x].value_counts(normalize = False))
    raw_counts = raw_counts.rename(columns = {x: 'raw_count'})
    
    # Calculate counts for each group of x and y
    counts = pd.DataFrame(data.groupby(y)[x].value_counts(normalize = True))
    
    # Rename the column and reset the index
    counts = counts.rename(columns = {x: 'normalized_count'}).reset_index()
    counts['percent'] = 100 * counts['normalized_count']
    
    # Add the raw count
    counts['raw_count'] = list(raw_counts['raw_count'])
    
    plt.figure(figsize = (14, 10))
    # Scatter plot sized by percent
    plt.scatter(counts[x], counts[y], edgecolor = 'k', color = 'lightgreen',
                s = 100 * np.sqrt(counts['raw_count']), marker = 'o',
                alpha = 0.6, linewidth = 1.5)
    
    if annotate:
        # Annotate the plot with text
        for i, row in counts.iterrows():
            # Put text with appropriate offsets
            plt.annotate(xy = (row[x] - (1 / counts[x].nunique()), 
                               row[y] - (0.15 / counts[y].nunique())),
                         color = 'navy',
                         s = f"{round(row['percent'], 1)}%")
        
    # Set tick marks
    plt.yticks(counts[y].unique())
    plt.xticks(counts[x].unique())
    
    # Transform min and max to evenly space in square root domain
    sqr_min = int(np.sqrt(raw_counts['raw_count'].min()))
    sqr_max = int(np.sqrt(raw_counts['raw_count'].max()))
    
    # 5 sizes for legend
    msizes = list(range(sqr_min, sqr_max,
                        int(( sqr_max - sqr_min) / 5)))
    markers = []
    
    # Markers for legend
    for size in msizes:
        markers.append(plt.scatter([], [], s = 100 * size, 
                                   label = f'{int(round(np.square(size) / 100) * 100)}', 
                                   color = 'lightgreen',
                                   alpha = 0.6, edgecolor = 'k', linewidth = 1.5))
        
    # Legend and formatting
    plt.legend(handles = markers, title = 'Counts',
               labelspacing = 3, handletextpad = 2,
               fontsize = 16,
               loc = (1.10, 0.19))
    
    plt.annotate(f'* Size represents raw count while % is for a given y value.',
                 xy = (0, 1), xycoords = 'figure points', size = 10)
    
    # Adjust axes limits
    plt.xlim((counts[x].min() - (6 / counts[x].nunique()), 
              counts[x].max() + (6 / counts[x].nunique())))
    plt.ylim((counts[y].min() - (4 / counts[y].nunique()), 
              counts[y].max() + (4 / counts[y].nunique())))
    plt.grid(None)
    plt.xlabel(f"{x}"); plt.ylabel(f"{y}"); plt.title(f"{y} vs {x}");
    
# Read Data    
train=pd.read_csv(r'D:\projects\poverty_prediciton\train.csv')
test=pd.read_csv(r'D:\projects\poverty_prediciton\test.csv')


#Set styles
plt.style.use('fivethirtyeight')
plt.rcParams['font.size'] = 5
plt.rcParams['patch.edgecolor'] = 'k'

#analyse int
analyse_int(train)
analyse_float(train)


#analyse object
train.select_dtypes('object').head()
#Clearly some columns are mix of numbers and chars
train['dependency'].value_counts()
train['edjefe'].value_counts()
train['edjefa'].value_counts()



#Replace yes and no
mapping={'yes':1,'no':0}
for df in [train,test]:
    df['dependency']=df['dependency'].replace(mapping).astype(np.float)
    df['edjefe']=df['edjefe'].replace(mapping).astype(np.float)
    df['edjefa']=df['edjefa'].replace(mapping).astype(np.float)


#See their distributions
analyse_float(train)



#Merge train and test
test['Target']=np.nan
data=train.append(test,ignore_index=True)

#Analyse target
analyse_target(data)

#Since this is an aggregation problem we have to check that the values remain consistent across every household
#1328 with single records

check_consistency(data)

#Families without heads
household_head_count=data.groupby('idhogar')['parentesco1'].apply(lambda x: x.sum()<1)
nohead_list=household_head_count[household_head_count==True].index.tolist()


#Missing values
get_missing(data)
data=fix_missing(data)


plot_categoricals('rez_esc', 'Target', data)
plot_categoricals('escolari','Target',data)   












#Feature Engineering
id_ = ['Id', 'idhogar', 'Target']
ind_bool = ['v18q', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', 
            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', 
            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', 
            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', 
            'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', 
            'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 
            'instlevel9', 'mobilephone']

ind_ordered = ['rez_esc', 'escolari', 'age']
hh_bool = ['hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', 
           'paredpreb','pisocemento', 'pareddes', 'paredmad',
           'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', 
           'pisonatur', 'pisonotiene', 'pisomadera',
           'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', 
           'abastaguadentro', 'abastaguafuera', 'abastaguano',
            'public', 'planpri', 'noelec', 'coopele', 'sanitario1', 
           'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6',
           'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4', 
           'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', 
           'elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3',
           'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', 
           'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', 
           'computer', 'television', 'lugar1', 'lugar2', 'lugar3',
           'lugar4', 'lugar5', 'lugar6', 'area1', 'area2']

hh_ordered = [ 'rooms', 'r4h1', 'r4h2', 'r4h3', 'r4m1','r4m2','r4m3', 'r4t1',  'r4t2', 
              'r4t3', 'v18q1', 'tamhog','tamviv','hhsize','hogar_nin',
              'hogar_adul','hogar_mayor','hogar_total',  'bedrooms', 'qmobilephone']

hh_cont = ['v2a1', 'dependency', 'edjefe', 'edjefa', 'meaneduc', 'overcrowding']
sqr_ = ['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', 
        'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq']

total_lists=[id_,ind_bool,ind_ordered,hh_bool,hh_ordered,hh_cont,sqr_]
x = ind_bool + ind_ordered + id_ + hh_bool + hh_ordered + hh_cont + sqr_
from collections import Counter

print('There are no repeats: ', np.all(np.array(list(Counter(x).values())) == 1))
print('We covered every variable: ', len(x) == data.shape[1])

#What about the squared variables
sns.lmplot('age','SQBage',data=data,fit_reg=False)
#These variables are highly correlated and we can't keep both in our data
data.drop(sqr_,1,inplace=True)
data.shape


#We will keep the id variables as is for now
heads = data.loc[data['parentesco1'] == 1, :]
heads = heads[id_ + hh_bool + hh_cont + hh_ordered]
heads.shape

#Find columns having large correlations
corr_matrix=heads.corr()
upper=corr_matrix.where(np.triu(np.ones(corr_matrix.shape),k=1).astype(np.bool))
drop_cols=[column for column in upper.columns if any(abs(upper[column])>0.95)]

#Find the other pair of columns
corr_matrix.loc[corr_matrix.tamhog.abs()>0.95,corr_matrix.tamhog.abs()>0.95]
heads = heads.drop(columns = ['tamhog', 'hogar_total', 'r4t3'])
drop_lists=['tamhog','r4t3','hogar_total']

sns.lmplot('tamviv','hhsize',data=heads,fit_reg=False)
heads['hhsize-diff']=heads['tamviv']-heads['hhsize']
plot_categoricals('hhsize-diff','Target',heads)

corr_matrix.loc[corr_matrix['coopele'].abs()>0.9,corr_matrix['coopele'].abs()>0.9]

#Using columns noelec,planpri,coopele,public make a new variable
def combine_elec_vars(row):
    if(row['noelec']==1):
        return 0
    elif(row['public']==1):
        return 1
    elif(row['planpri']==1):
        return 3
    elif(row['coopele']==1):
        return 2
    
heads['elec']=heads.apply(combine_elec_vars,1)
heads.drop(['noelec','public','planpri','coopele'],1,inplace=True)

drop_lists=drop_lists+['noelec','public','planpri','coopele']
plot_categoricals('elec','Target',heads)

#Similarly create ordinal variables using 'etecho1', 'etecho2', 'etecho3'--using argmax
heads['roof']=np.argmax(np.array(heads[['etecho1', 'etecho2', 'etecho3']]),axis=1)
plot_categoricals('roof','Target',heads)

heads['floor'] = np.argmax(np.array(heads[['eviv1', 'eviv2', 'eviv3']]),axis = 1)
plot_categoricals('floor','Target',heads)

heads['walls']= np.argmax(np.array(heads[['epared1', 'epared2', 'epared3']]),axis = 1)
plot_categoricals('Target','walls',heads)

heads.drop(['etecho1', 'etecho2', 'etecho3','eviv1', 'eviv2', 'eviv3','epared1', 'epared2', 'epared3'],axis=1,inplace=True)
drop_lists=drop_lists+['etecho1', 'etecho2', 'etecho3','eviv1', 'eviv2', 'eviv3','epared1', 'epared2', 'epared3']

#make a new veariable
heads['walls+roof+floor']=heads['walls']+heads['roof']+heads['floor']
plot_categoricals('walls+roof+floor','Target',heads)
# Lets not drop this for now

#Per capita features
heads['phones-per-capita'] = heads['qmobilephone'] / heads['tamviv']
heads['tablets-per-capita'] = heads['v18q1'] / heads['tamviv']
heads['rooms-per-capita'] = heads['rooms'] / heads['tamviv']
heads['rent-per-capita'] = heads['v2a1'] / heads['tamviv']

#Bonus features
# Owns a refrigerator, computer, tablet, and television
heads['bonus'] = 1 * (heads['refrig'] + 
                      heads['computer'] + 
                      (heads['v18q1'] > 0) + 
                      heads['television'])

heads['warning'] = 1 * (heads['sanitario1'] + 
                         (heads['elec'] == 0) + 
                         heads['pisonotiene'] + 
                         heads['abastaguano'] + 
                         (heads['cielorazo'] == 0))

plt.figure(figsize = (10, 6))
sns.violinplot(x = 'warning', y = 'Target', data = heads);
plt.title('Target vs Warning Variable');

plot_categoricals('warning','Target',heads)


#More features
heads['phones-per-capita'] = heads['qmobilephone'] / heads['tamviv']
heads['tablets-per-capita'] = heads['v18q1'] / heads['tamviv']
heads['rooms-per-capita'] = heads['rooms'] / heads['tamviv']
heads['rent-per-capita'] = heads['v2a1'] / heads['tamviv']

# Now let's find the spearman and pearson correlation
from scipy.stats import spearmanr
def plot_corrs(x, y):
    """Plot data and show the spearman and pearson correlation."""
    
    # Calculate correlations
    spr = spearmanr(x, y).correlation
    pcr = np.corrcoef(x, y)[0, 1]
    
    # Scatter plot
    data = pd.DataFrame({'x': x, 'y': y})
    plt.figure( figsize = (6, 4))
    sns.regplot('x', 'y', data = data, fit_reg = False);
    plt.title(f'Spearman: {round(spr, 2)}; Pearson: {round(pcr, 2)}');
    

#For individual levels
ind = data[id_ + ind_bool + ind_ordered]
ind.shape


#Redundant Individual variables
corr_matrix = ind.corr()

# Select upper triangle of correlation matrix
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))

# Find index of feature columns with correlation greater than 0.95
to_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]

to_drop
#Because of male, so drop male
ind = ind.drop(columns = 'male')



#Creating ordinal variables
ind[[c for c in ind if c.startswith('instl')]].head()
ind['inst'] = np.argmax(np.array(ind[[c for c in ind if c.startswith('instl')]]), axis = 1)
cols=ind[[c for c in ind if c.startswith('instl')]].columns.tolist()
ind=ind.drop(cols,1)
drop_lists=drop_lists+cols


plot_categoricals('inst', 'Target', ind);


plt.figure(figsize = (10, 8))
sns.violinplot(x = 'Target', y = 'inst', data = ind);
plt.title('Education Distribution by Target')


#Feature construction
ind['escolari/age'] = ind['escolari'] / ind['age']
plt.figure(figsize = (10, 8))
sns.violinplot('Target', 'escolari/age', data = ind)

ind['inst/age'] = ind['inst'] / ind['age']
ind['tech'] = ind['v18q'] + ind['mobilephone']
ind['tech'].describe()



#Aggregate using stats
range_ = lambda x: x.max() - x.min()
range_.__name__ = 'range_'
# Group and aggregate
ind_agg = ind.drop(columns = 'Target').groupby('idhogar').agg(['min', 'max', 'sum', 'count', 'std', range_])
ind_agg.head()


new_col = []
for c in ind_agg.columns.levels[0]:
    for stat in ind_agg.columns.levels[1]:
        new_col.append(f'{c}-{stat}')
        
ind_agg.columns = new_col
ind_agg.head()



#Feature selection

corr_matrix = ind_agg.corr()

# Select upper triangle of correlation matrix
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))

# Find index of feature columns with correlation greater than 0.95
to_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]

print(f'There are {len(to_drop)} correlated columns to remove.')

#Drop highly correlated features
ind_agg = ind_agg.drop(columns = to_drop)
ind_feats = list(ind_agg.columns)


final=heads.merge(ind_agg,on='idhogar',how='left')

